{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our SparkSession so we can use it\n",
    "from pyspark.sql import SparkSession, SQLContext, functions as F\n",
    "# Create our SparkSession, this can take a couple minutes locally\n",
    "spark = SparkSession.builder.appName(\"main\").config('spark.sql.broadcastTimeout',-1).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business = spark.read.json(\"../data_source/business.json\")\n",
    "df_reviews = spark.read.json(\"../data_source/review.json\")\n",
    "df_users = spark.read.json(\"../data_source/user.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_users.groupBy(\"compliment_cool\").count().show(100, False)\n",
    "\n",
    "df_users.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business.filter(\"state == 'NV'\").select(\"business_id\", \"name\").show(50, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business.filter(df_business.business_id == '3de3H2nR8RN2TKSJi-vVuQ').select(\"name\").show(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# join_table = df_business.alias('business').join(df_reviews.alias('review'), col('review.business_id') == col('business.business_id')).select([col('business.business_id')])\n",
    "# join_table = df_business.alias('business').join(df_reviews.alias('review'), col('review.business_id') == col('business.business_id')).select([col('business.'+xx) for xx in df_business.columns])\n",
    "\n",
    "join_table = df_business.alias('business').join(df_reviews.alias('review'), col('review.business_id') == col('business.business_id')).select([col('business.'+xx) for xx in df_business.columns] + [col('review.business_id'), col('review.user_id'),col('review.review_id'), col('review.stars'), col('review.text')])\n",
    "# join_table = df_reviews.alias('review').join(df_business.alias('business'), col('business.business_id') == col('review.business_id')).select([col('review.'+xx) for xx in df_reviews.columns] + [col('business.categories'), col('state')])\n",
    "# join_table = df_business.join(df_reviews, 'business_id')\n",
    "\n",
    "# spark.read.json(\"raw_data/business.json\")\n",
    "\n",
    "# def saveToMapRDB(dataframe, table_name, id_field_path = default_id_field, create_table = False, bulk_insert = False)\n",
    "              \n",
    "# from pyspark.sql import SparkSession \n",
    "\n",
    "# df = spark.loadFromMapRDB(\"/tmp/user_profiles\")\n",
    "            \n",
    "# SparkSession.saveToMapRDB(join_table, 'business', create_table=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "join_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "join_table.selectExpr(\"business.*\", \"review.user_id\", \"review.review_id\", \"review.stars as rev_stars\", \"review.text\").coalesce(1).write.format('json').save('raw_data/business_review')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# table = join_table.where(join_table.business_id == \"1SWheh84yJXfytovILXOAQ\").select(join_table.categories)\n",
    "\n",
    "join_table.filter(col('business.business_id') == \"--9e1ONYQuAa-CB_Rrw7Tw\").select(join_table.categories).show(3, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # join_table.describe(\"categories\").count()\n",
    "\n",
    "# # join_table.registerAsTable('train_table')\n",
    "# df_business.registerAsTable('train_table')\n",
    "\n",
    "\n",
    "df_business.createOrReplaceTempView(\"data_super\")\n",
    "# import pyspark.sql.functions as F\n",
    "# from pyspark.sql.types import LongType\n",
    "# import copy\n",
    "\n",
    "# X = spark.createDataFrame([[1,2], [3,4]], ['a', 'b'])\n",
    "# _schema = copy.deepcopy(df_business.schema)\n",
    "# ['address',\n",
    "#  'attributes',\n",
    "#  'business_id',\n",
    "#  'categories',\n",
    "#  'city',\n",
    "#  'hours',\n",
    "#  'is_open',\n",
    "#  'latitude',\n",
    "#  'longitude',\n",
    "#  'name',\n",
    "#  'postal_code',\n",
    "#  'review_count',\n",
    "#  'stars',\n",
    "#  'state']\n",
    "\n",
    "# _schema.remove('address')\n",
    "\n",
    "\n",
    "# .add('id_col', LongType(), False) # modified inplace\n",
    "# _X = X.rdd.zipWithIndex().map(lambda l: list(l[0]) + [l[1]]).toDF(_schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_resultsq = spark.sql(\"SELECT * FROM data_super\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business.groupBy(\"categories\").count().show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = df_business.select('business_id', 'categories')\n",
    "table2 = df_reviews.select('business_id', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_business.join(table2, \"business_id\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# results = df.where(F.col(\"categories\").isNotNull()).select(df.business_id)\n",
    "\n",
    "# # pd.DataFrame([json.load(results)])\n",
    "# dataFrame = results.toPandas()\n",
    "\n",
    "# # dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.filter(df.business_id == \"gnKjwL_1w79qoiV3IC_xQQ\").select(\"attributes.BusinessParking\").show(50, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.where(F.col(\"attributes.BusinessParking\").isNotNull()).select(\"attributes.BusinessParking\").show(50, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.where(F.col(\"categories\").isNotNull()).select(df.business_id, df.categories).show(50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "# from pyspark.mllib.util import MLUtils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# X = np.random.randint(5, size=(6, 100))\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X[2:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
